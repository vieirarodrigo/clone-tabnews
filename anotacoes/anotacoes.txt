### Git e GitHub

# O que é Git?

O Git é um sistema de controle de versão distribuído, gratuito e de código aberto, que permite rastrear e gerenciar as alterações em um projeto de software ao longo do tempo. Ele foi criado por Linus Torvalds em 2005. O Git é uma ferramenta essencial para desenvolvedores, facilitando a colaboração em projetos, a criação de versões e a reversão de alterações caso necessário. 

O Git é como se fosse um vídeo no meu computador.

# O que é GitHub?

O GitHub é uma plataforma online que funciona como um grande repositório de código de programação, onde desenvolvedores podem armazenar, compartilhar e colaborar em projetos. É essencialmente uma rede social para programadores, facilitando o trabalho em equipe e a colaboração em projetos de código aberto. 

O GitHub é como o YouTube, uma plataforma que posso compartilhar ou assistir outros videos.

Para criar uma conta no GitHub: https://docs.github.com/pt/get-started/start-your-journey/creating-an-account-on-github

Para criar um repositório no GitHub: https://docs.github.com/pt/repositories/creating-and-managing-repositories/creating-a-new-repository

### Codespaces

# O que é codespaces?

Um codespace é um ambiente de desenvolvimento virtual e hospedado na nuvem, oferecido pelo GitHub. Permite que os desenvolvedores escrevam, testem e depurem código diretamente na nuvem, sem a necessidade de configurações locais complexas. 

Como criar um codespace para um repositório: https://docs.github.com/pt/codespaces/developing-in-a-codespace/creating-a-codespace-for-a-repository

### Atenção

Aprenda a função das coisas, não se preenda a gostos pessoais.

### Anotações pessoais

O Git é um controlador de versões, ele cria e gerencia várias versões do projeto. Ele faz uma cópia sempre que você o chama, não copiando totalmente, mas indicando as alterações que foram realizadas em cada versão.

O GitHub é uma rede social, no qual posso compartilhar projetos, acessar, editar, copiar e interagir com projetos de outras pessoas.

O codespaces é um espaço para eu desenvolver meus projetos, ele cria uma máquina virtual com o vscode e um terminal, no qual me permite desenvolver sem as limitações de um computador físico mais modesto.

### Referências:

https://www.atlassian.com/br/git/tutorials/what-is-git
https://docs.github.com/pt/get-startedstart-your-journey/about-github-and-git
https://docs.github.com/pt/codespaces/about-codespaces/what-are-codespaces

#################################

"Se você quiser fazer uma torta de maçã a partir do zero, você deve primeiro inventar o Universo." - Carl Sagan

Não é possível hoje começar algo do zero, vamos precisar subir no ombro de gigantes e alçar novos voos. Isso não é um problema, podemos fazer coisas incríveis em comunidade.

### Node.js

# O que é node.js

Node.js é um ambiente de execução JavaScript do lado do servidor, que permite aos desenvolvedores criar aplicações web, APIs, e outras ferramentas de linha de comando utilizando a linguagem JavaScript.

Node.js é como uma cozinha que permite preparar receitas (aplicações) usando um fogão (ambiente de execução) que entende a linguagem do JavaScript.

# Terminal 

Terminal é uma interface de linha de comando que permite interagir com um sistema operacional através de comandos digitados, sme a necissade de uma interface gráfica.

Terminal é como um pedaço de papel, ele permite que você intereja com o sistema operacional, enquanto o shell é o "motor" que interpreta e executa seus comandos.

# Shell

É uma interface que permite aos utilizadores interagir com um sistema operacional.

É como a casca de um ovo, que protege o conteúdo interno.

# NVM (Node Version Manager)

NVM é uma ferramenta que permite aos desenvolvedores gerenciar várias cersões do Node.js em um único sistema operacional.

# Para acessar os detalhes do NVM use:

nvm --help (node version manager ajuda)

### Como instalar uma versão específica do Node.js?

1º Vamos precisar listar as versões disponiveis,vamos usar o comando: 

nvm ls (node version manager list)

2º Escolha a versão, por exemplo lts/hydrogen, para isso use os comandos:

nvm install lts/hydrogen (node version manager install version)

### Como deixar a versão anterior como padrão

Para isso vamos precisar definir como padrão a versão deseja, conforme abaixo:

nvm alias default <version> (node version manager apelido padrão versão)

### Declarar a versão utilizada no projeto

Para deixar claro para outras pessoas e padronizar o projeto, precisamos deixar claro qual versão do node vai ser utilizado, fazemos isso da forma abaixo:

1º Crie o arquivo abaixo na pasta raiz do projeto.
.nvmrc (node version manager run commands)

2º Dentro escreva a versão desejada e pule uma linha.

Exemplo:
"
lts/hydrogen

"

### Next.js

É um framework React para construir aplicações web, que fornece uma estrutura e recursos extras para otimizar o desenvolvimento e melhorar o desempenho das aplicações.

É como um motor que facilita a construção de aplicações web com React.

Também para o Next e o React, é necessário indicar no projeto quais versões foram utilizadas. Para isso vamos:

1º Precisamos inicializar o node no nosso projeto e esse comando vai criar um arquivo package.json, usamos o comando:

npm init (node package manager initialize)

2º Vamos escolher a versão do next.js que será utilizado, nesse exemplo vamos usar a versão 13.1.6:

npm install next@13.1.6 (node package manager install <version>)

### React

É uma biblioteca JavaScript de software livre, popularmente utilizada para construir interfaces de usuário do lado do cliente (front-end).

É como um sistema de blocos de construção para interfaces de usuário, onde cada bloco (componente) pode ser utilizado e combinado para criar aplicações complexas.

É necessário instalar a versão do react, nesse exemplo é a 18.2.0:

npm install react@18.2.0 (node package install <version>)

O ponto é que o React foi separado entre seu core e suas renderizações, pois ele é extremamente vasto.

Para HTML, quem renderiza é o DOM, para isso vamos instalar a versão do react DOM, para manipula-lo:

npm install react-dom@18.2.0 (node package install <version>)






O que é um serviço web?

Protocolos

HTTP
FTP
SMTP
UDP
IP

UDP TCP

next dev
"dev": "next dev"


Ctrl + L

pages

index.js 

function Home() {
    return <h1>Teste</h1>
}

export default Home;

deploy



#################
Control History

Versionamento de código

sccs -> rcs -> cvs -> svn -> git
       centralizado        | distribuído

Merge

Merge conflict

Clone

ls -la

.git

Diff

Delta encoding

Blob

git log

Commit


3 estágios

1 - Modified
2 - Staged
3 - Commit

git log --stat

git status

Unstrack file

Build

.gitignore

git add nome do arquivo

git commit



git log --oneline

Working Directory

Diff

git diff

Newline

Amend

git commit --amend 

Gits são imutáveis












origin/main

branch

git push

pull

git commit -m "Mensagem"

git push --force ou --f







Continuos deployment

Client / Server


Hospedagem de site

Deploy

Continuos integration -> Build


Principle least privilege

Cada commit deployado, kkk, tem uma url unica









N1: Ser lembrado indivualmente; (Anotar tarefas no papel, em cima da mesa)
N2: Ser lembrado em grupo; (Quadro kambam)
N3: Expandir conhecimento; (Trello ou Github)
N4: Gerar metas. (Jira)


Issue inception

Milestones
    - Milestone 0: Em construção

Estágio 1 - Início
Estágio 2 - Progresso
Estágio 3 - Conclusão

Issues: 

Colocar o site num domínio .com.br
Definir estilização do código e configurar editor
Programar página de "Em construção"










Estilização de código é essencial

- [ ] - Ligar sincronização do editor
- [ ] - Configurar o EditorConfig
- [ ] - Configurar o Prettier


EditorConfig

.EditorConfig

root = true

[*] 
indent_style = space
indent_size = 2


Prettier / Standard js

npm install prettier -D ou --save-dev


no package.json
"lint:check": "prettier --check ."
"lint:fix": "prettier --write ."







DNS 2 - níveis

IP é o endereço de cada computador

DNS é um apelido

DNS <- Computador -> servidor
    ->

Recursive Resolver <- Computador
                    ->

Recursive Resolver -> Root Server
                   <-
                   -> TLD 
                   <-
                   -> Authoritative Server

FQDN

Root Domain - TLD - Authoritative Server - TTL
            ccTLD/gTLD












### Dia 12 - DNS (Prática)

Task da insue "Coloca o site num domínio .com.br"

- [ ] - Registrar domínio próprio
- [ ] - Configurar Servidor de DNS

Registrant -> Registrar -> Registry -> TLD

whatismydns -> NS

vieirarodrigo.com.br


Dizer para vercel o nosso domínio

Dizer para a TLD o nosso DNS


### Git e GitHub

# O que é Git?

O Git é um sistema de controle de versão distribuído, gratuito e de código aberto, que permite rastrear e gerenciar as alterações em um projeto de software ao longo do tempo. Ele foi criado por Linus Torvalds em 2005. O Git é uma ferramenta essencial para desenvolvedores, facilitando a colaboração em projetos, a criação de versões e a reversão de alterações caso necessário. 

O Git é como se fosse um vídeo no meu computador.

# O que é GitHub?

O GitHub é uma plataforma online que funciona como um grande repositório de código de programação, onde desenvolvedores podem armazenar, compartilhar e colaborar em projetos. É essencialmente uma rede social para programadores, facilitando o trabalho em equipe e a colaboração em projetos de código aberto. 

O GitHub é como o YouTube, uma plataforma que posso compartilhar ou assistir outros videos.

Para criar uma conta no GitHub: https://docs.github.com/pt/get-started/start-your-journey/creating-an-account-on-github

Para criar um repositório no GitHub: https://docs.github.com/pt/repositories/creating-and-managing-repositories/creating-a-new-repository

### Codespaces

# O que é codespaces?

Um codespace é um ambiente de desenvolvimento virtual e hospedado na nuvem, oferecido pelo GitHub. Permite que os desenvolvedores escrevam, testem e depurem código diretamente na nuvem, sem a necessidade de configurações locais complexas. 

Como criar um codespace para um repositório: https://docs.github.com/pt/codespaces/developing-in-a-codespace/creating-a-codespace-for-a-repository

### Atenção

Aprenda a função das coisas, não se preenda a gostos pessoais.

### Anotações pessoais

O Git é um controlador de versões, ele cria e gerencia várias versões do projeto. Ele faz uma cópia sempre que você o chama, não copiando totalmente, mas indicando as alterações que foram realizadas em cada versão.

O GitHub é uma rede social, no qual posso compartilhar projetos, acessar, editar, copiar e interagir com projetos de outras pessoas.

O codespaces é um espaço para eu desenvolver meus projetos, ele cria uma máquina virtual com o vscode e um terminal, no qual me permite desenvolver sem as limitações de um computador físico mais modesto.

### Referências:

https://www.atlassian.com/br/git/tutorials/what-is-git
https://docs.github.com/pt/get-startedstart-your-journey/about-github-and-git
https://docs.github.com/pt/codespaces/about-codespaces/what-are-codespaces

#################################

"Se você quiser fazer uma torta de maçã a partir do zero, você deve primeiro inventar o Universo." - Carl Sagan

Não é possível hoje começar algo do zero, vamos precisar subir no ombro de gigantes e alçar novos voos. Isso não é um problema, podemos fazer coisas incríveis em comunidade.

### Node.js

# O que é node.js

Node.js é um ambiente de execução JavaScript do lado do servidor, que permite aos desenvolvedores criar aplicações web, APIs, e outras ferramentas de linha de comando utilizando a linguagem JavaScript.

Node.js é como uma cozinha que permite preparar receitas (aplicações) usando um fogão (ambiente de execução) que entende a linguagem do JavaScript.

# Terminal 

Terminal é uma interface de linha de comando que permite interagir com um sistema operacional através de comandos digitados, sme a necissade de uma interface gráfica.

Terminal é como um pedaço de papel, ele permite que você intereja com o sistema operacional, enquanto o shell é o "motor" que interpreta e executa seus comandos.

# Shell

É uma interface que permite aos utilizadores interagir com um sistema operacional.

É como a casca de um ovo, que protege o conteúdo interno.

# NVM (Node Version Manager)

NVM é uma ferramenta que permite aos desenvolvedores gerenciar várias cersões do Node.js em um único sistema operacional.

# Para acessar os detalhes do NVM use:

nvm --help (node version manager ajuda)

### Como instalar uma versão específica do Node.js?

1º Vamos precisar listar as versões disponiveis,vamos usar o comando: 

nvm ls (node version manager list)

2º Escolha a versão, por exemplo lts/hydrogen, para isso use os comandos:

nvm install lts/hydrogen (node version manager install version)

### Como deixar a versão anterior como padrão

Para isso vamos precisar definir como padrão a versão deseja, conforme abaixo:

nvm alias default <version> (node version manager apelido padrão versão)

### Declarar a versão utilizada no projeto

Para deixar claro para outras pessoas e padronizar o projeto, precisamos deixar claro qual versão do node vai ser utilizado, fazemos isso da forma abaixo:

1º Crie o arquivo abaixo na pasta raiz do projeto.
.nvmrc (node version manager run commands)

2º Dentro escreva a versão desejada e pule uma linha.

Exemplo:
"
lts/hydrogen

"

### Next.js

É um framework React para construir aplicações web, que fornece uma estrutura e recursos extras para otimizar o desenvolvimento e melhorar o desempenho das aplicações.

É como um motor que facilita a construção de aplicações web com React.

Também para o Next e o React, é necessário indicar no projeto quais versões foram utilizadas. Para isso vamos:

1º Precisamos inicializar o node no nosso projeto e esse comando vai criar um arquivo package.json, usamos o comando:

npm init (node package manager initialize)

2º Vamos escolher a versão do next.js que será utilizado, nesse exemplo vamos usar a versão 13.1.6:

npm install next@13.1.6 (node package manager install <version>)

### React

É uma biblioteca JavaScript de software livre, popularmente utilizada para construir interfaces de usuário do lado do cliente (front-end).

É como um sistema de blocos de construção para interfaces de usuário, onde cada bloco (componente) pode ser utilizado e combinado para criar aplicações complexas.

É necessário instalar a versão do react, nesse exemplo é a 18.2.0:

npm install react@18.2.0 (node package install <version>)

O ponto é que o React foi separado entre seu core e suas renderizações, pois ele é extremamente vasto.

Para HTML, quem renderiza é o DOM, para isso vamos instalar a versão do react DOM, para manipula-lo:

npm install react-dom@18.2.0 (node package install <version>)






O que é um serviço web?

Protocolos

HTTP
FTP
SMTP
UDP
IP

UDP TCP

next dev
"dev": "next dev"


Ctrl + L

pages

index.js 

function Home() {
    return <h1>Teste</h1>
}

export default Home;

deploy



#################
Control History

Versionamento de código

sccs -> rcs -> cvs -> svn -> git
       centralizado        | distribuído

Merge

Merge conflict

Clone

ls -la

.git

Diff

Delta encoding

Blob

git log

Commit


3 estágios

1 - Modified
2 - Staged
3 - Commit

git log --stat

git status

Unstrack file

Build

.gitignore

git add nome do arquivo

git commit



git log --oneline

Working Directory

Diff

git diff

Newline

Amend

git commit --amend 

Gits são imutáveis












origin/main

branch

git push

pull

git commit -m "Mensagem"

git push --force ou --f







Continuos deployment

Client / Server


Hospedagem de site

Deploy

Continuos integration -> Build


Principle least privilege

Cada commit deployado, kkk, tem uma url unica









N1: Ser lembrado indivualmente; (Anotar tarefas no papel, em cima da mesa)
N2: Ser lembrado em grupo; (Quadro kambam)
N3: Expandir conhecimento; (Trello ou Github)
N4: Gerar metas. (Jira)


Issue inception

Milestones
    - Milestone 0: Em construção

Estágio 1 - Início
Estágio 2 - Progresso
Estágio 3 - Conclusão

Issues: 

Colocar o site num domínio .com.br
Definir estilização do código e configurar editor
Programar página de "Em construção"










Estilização de código é essencial

- [ ] - Ligar sincronização do editor
- [ ] - Configurar o EditorConfig
- [ ] - Configurar o Prettier


EditorConfig

.EditorConfig

root = true

[*] 
indent_style = space
indent_size = 2


Prettier / Standard js

npm install prettier -D ou --save-dev


no package.json
"lint:check": "prettier --check ."
"lint:fix": "prettier --write ."







DNS 2 - níveis

IP é o endereço de cada computador

DNS é um apelido

DNS <- Computador -> servidor
    ->

Recursive Resolver <- Computador
                    ->

Recursive Resolver -> Root Server
                   <-
                   -> TLD 
                   <-
                   -> Authoritative Server

FQDN

Root Domain - TLD - Authoritative Server - TTL
            ccTLD/gTLD












### Dia 12 - DNS (Prática)

Task da insue "Coloca o site num domínio .com.br"

- [ ] - Registrar domínio próprio
- [ ] - Configurar Servidor de DNS

Registrant -> Registrar -> Registry -> TLD

whatismydns -> NS

vieirarodrigo.com.br


Dizer para vercel o nosso domínio

Dizer para a TLD o nosso DNS


dig site A +trace





###  Dia 13 - 

As pessoas estão se aproximando ou se afastando?

Teoria McDonalds


99.9% Uptime
9h/Ano ou 44min/mês

SLA

Status Pages

vercel status ou aws status ou github status

RDS






### Dia 14 - Milestone 1: Fundação

## D1 - Inauguração Milestone 1: Fundação

Front-end

Milestone 1: Fundação
  Proposta de arquitetura e pastas
  Testes automatizados
  Banco de dados (Local)
  Migrations
  Continuos Integration
  Linter de código
  Linter de commits
  Banco de dados (homologação e produção)
  Tipo da licença


## D2 - Uma história macabra sobre "Overengineering"

Overengineering - Excesso de engenharia (Complexo)

Você quer estar aqui

Underengineering - (Simples e mal feito)

Um software deve ser modificável


## D3 - Porposta de Arquitetura e pastas

1. Arquitetura de software

MVC - Model View Controler

2. Organização das pastas e arquivos

root
  pages
    index.js
  models
    user.js
    content.js
    password.js
  infra
    database.js
    migration
    provisioning
      staging
      production
  tests

## D4 - Bônus: PoC e MVP ajudam mesmo?

PoC - Proof of Concept
MVP - Minimum Viable Product




### Dia 15 

## D1 - Testes Automatizados: um caminho sem volta

62 Aulas??

- [ ] - Instalar o Test Runner
- [ ] - Criar um Teste de Teste
- [ ] - Criar um Teste de Verdade

## D2 - Instalar um Test Runner

Código que executa Código

Mocha | AVA | Playwight | Jest

npm install --save-dev jest@29.6.2

"test": "jest"
"test:watch": "jest --watch"


## D3 - Criar um "Teste de Teste"

Programar uma calculadora

npm run test -watch

tests

calculadora.test.js

test("nome do teste", callbackFunction);

function callbackFunction() {
  console.log("esta função está sendo chamada?")
}

test("nome do teste", function () {
  console.log("e assim, funciona?");
})

test("nome do teste", () => {
  console.log("e agora?");
});

test("outro teste", () => {
  console.log("outro teste");
});

Expect - Espera

test("espero que 1 seja 1", () => {
  espect(1).toBe(1);
});

Valor gerado dinamicamente | Valor esperado




## D4 - Criar um "Teste de Verdade"

No MVC

Criar ferramentas (Model)

CommonJS (ESM)

Trasnpilling

expect(resultado)
Softcoded

.toBe(4)
Hardcoded

Teste não diz que o codigo está certo, mas que algo passou ou não em um teste

TDD





Resumo: 

E aí, pessoal do Curso.Dev! Sejam muito bem-vindos a mais uma aula que vai mudar completamente a forma como vocês encaram o desenvolvimento de software. Eu sei que o Felipe já tocou na importância disso, mas hoje, a gente vai mergulhar de cabeça nos **Testes Automatizados: um caminho sem volta**! Preparem-se para uma sacada que vai acelerar a carreira de vocês.

Pensem comigo: no desenvolvimento de software moderno, testar manualmente cada alteração de código é como tentar esvaziar o Oceano Atlântico com um balde. Inviável e ineficiente, né? É por isso que os testes automatizados são um pilar fundamental, proporcionando confiança, segurança e agilidade para o projeto. Eles garantem que, a cada nova funcionalidade ou correção, o comportamento esperado da aplicação seja verificado de forma automática.

Vamos estruturar nossa aula em três etapas essenciais, seguindo a didática que a gente tanto preza:

### 1. Instalar um Test Runner

Primeiro, a gente precisa de uma ferramenta para "rodar" nossos testes. Pensem no "Test Runner" como o maestro da orquestra dos seus testes. Ele é um código que procura os arquivos de teste no seu projeto, executa o que você escreveu e te apresenta um relatório claro do que passou e do que falhou. No ecossistema JavaScript, temos várias opções de peso no mercado, como Mocha, AVA, Playwright e, claro, o **Jest**.

O Jest, desenvolvido pelo Facebook e recomendado pela equipe do React, é um dos frameworks de teste mais populares e poderosos para projetos JavaScript. Por que ele? Simplicidade de configuração, execução rápida, mock integrado, suporte a ES6/TypeScript e uma comunidade ativa que é um verdadeiro *dream team* para tirar dúvidas.

**Como a gente instala o Jest?** Moleza! Primeiro, se você ainda não tem um projeto Node.js inicializado, comece com:

```bash
npm init -y
```
Depois, adicione o Jest como uma dependência de desenvolvimento (ou seja, ele só é necessário para quem vai desenvolver e testar, não para a versão final da aplicação):

```bash
npm install --save-dev jest
```
Isso instala a ferramenta que vai fazer a mágica acontecer.

Agora, uma **sacada de produtividade**: vamos configurar os scripts no seu `package.json` para facilitar a vida. Abra o seu `package.json` e adicione (ou ajuste) a seção `scripts` assim:

```json
// package.json
{
  ...
  "scripts": {
    "test": "jest",
    "test:watch": "jest --watch"
  },
  ...
}
```
Com `"test": "jest"`, você roda todos os testes do projeto uma vez com `npm test`. Já o `"test:watch": "jest --watch"` é um verdadeiro *game-changer* durante o desenvolvimento! Ele faz com que o Jest fique "escutando" as alterações nos seus arquivos e, ao salvar, roda automaticamente apenas os testes relevantes, dando um feedback instantâneo. Isso é agilidade pura!

### 2. Criar um "Teste de Teste" (Sanity Check)

Antes de testar a lógica complexa da sua aplicação, a gente sempre começa com um "Teste de Teste" ou um *sanity check*. O objetivo aqui é super simples: validar se o Jest foi configurado corretamente e se ele está conseguindo encontrar e executar os arquivos de teste. É a prova de vida do seu ambiente de testes!

Para isso, o Jest automaticamente reconhece arquivos com a extensão `.test.js` (ou `.spec.js`) como arquivos de teste. Uma estrutura comum para o seu projeto seria ter uma pasta `src` para o seu código e uma pasta `tests` para os seus testes.

Vamos criar um arquivo dentro da sua pasta `tests`, por exemplo, `meuprimeiro.test.js`:

```javascript
// meuprimeiro.test.js
test("espero que 1 seja 1", () => {
  // expect(valor_gerado).toBe(valor_esperado);
  expect(1).toBe(1);
});
```

A função `test()` define um teste individual, recebendo uma descrição em string e uma função *callback* com a lógica do teste. E o coração do teste? É a função `expect()` combinada com um "matcher" como o `.toBe()`. O `expect(valor_dinamico).toBe(valor_esperado)` é a forma de afirmar que o resultado do seu código (`valor_dinamico`) é exatamente igual ao que você esperava (`valor_esperado`).

Agora, execute seu "Teste de Teste" no terminal:

```bash
npm test
```

Se tudo deu certo, você verá uma saída parecida com esta:

```
PASS tests/meuprimeiro.test.js
✓ espero que 1 seja 1 (x ms)
```
Isso confirma que seu ambiente de testes está funcionando perfeitamente! Palmas para vocês!

### 3. Criar um "Teste de Verdade"

Com o ambiente validado, é hora de testar uma funcionalidade real da sua aplicação. Vamos pegar um exemplo clássico, como testar uma função `soma` em um arquivo `funcoes.js`.

Se você estiver seguindo padrões como MVC (Model-View-Controller), essa função estaria na sua camada de Model, onde ficam as regras de negócio.

Primeiro, crie o arquivo `src/funcoes.js` com a sua função:

```javascript
// src/funcoes.js
function soma(a, b) {
  return a + b;
}

module.exports = { soma }; // Usando CommonJS para exportar
```
**Atenção aqui!** Para o arquivo de teste poder acessar sua função `soma`, você precisa exportá-la. Usamos `module.exports` e `require` para módulos CommonJS, que é o padrão mais comum em projetos Node.js mais antigos e é o que o Jest usa por padrão. Se você estivesse em um projeto mais moderno com ES Modules (ESM), usaria `export` e `import`, mas isso pode exigir alguma configuração adicional, como um *transpiler* tipo Babel, para garantir compatibilidade com ambientes que ainda não suportam ESM nativamente. No nosso caso simples, `module.exports` e `require` funcionam direto com Jest.

Agora, crie o arquivo de teste `tests/funcoes.test.js`:

```javascript
// tests/funcoes.test.js
const { soma } = require('../src/funcoes'); // Importando a função a ser testada

test('soma dois números', () => {
  expect(soma(1, 2)).toBe(3);
});

test('soma números negativos', () => {
  expect(soma(-1, -2)).toBe(-3);
});

test('soma com zero', () => {
  expect(soma(5, 0)).toBe(5);
});
```

Execute novamente `npm test` e veja a mágica acontecer! Você verá todos os seus testes passando.

Uma "sacada" importantíssima sobre testes é a diferença entre valores "hardcoded" e "softcoded".
*   **Hardcoded (Valor Fixo):** É o valor que você escreve diretamente no teste como o resultado esperado. No exemplo `expect(resultado).toBe(4);`, o `4` é *hardcoded*. Isso é fundamental em testes unitários, pois você quer garantir que uma entrada específica sempre gere uma saída específica e previsível.
*   **Softcoded (Valor Dinâmico):** Refere-se ao valor gerado pelo seu código, como a variável `resultado` em `expect(resultado)`. Esse valor é dinâmico e será comparado com o valor *hardcoded* que você espera.

**Filosofia do Teste:** Entendam bem isso: um teste que passa *não prova* que seu código está 100% certo. Ele apenas prova que, para a *situação específica* que você testou, o código se comportou como o esperado. Se a sua função `soma` fosse `return 4;`, o teste `soma(1,2).toBe(3)` falharia, mas `soma(2,2).toBe(4)` passaria – o que mostra a necessidade de múltiplos casos de teste para ter confiança.

Isso nos leva a uma metodologia poderosa: o **TDD - Test-Driven Development (Desenvolvimento Guiado por Testes)**. O ciclo é simples, mas revolucionário:
1.  **Red (Vermelho):** Escreva um teste para uma funcionalidade que ainda não existe. Rode o teste e veja-o falhar.
2.  **Green (Verde):** Escreva o *mínimo* de código possível para fazer o teste passar.
3.  **Refactor (Refatorar):** Melhore o código que você escreveu sem alterar seu comportamento, garantindo que o teste continue passando.

Essa abordagem não só garante que seu código esteja coberto por testes desde o início, mas também incentiva a criação de um código mais simples, modular e fácil de manter!

Para ir além, vocês podem até gerar relatórios de **cobertura de código** com Jest. Basta adicionar `--coverage` ao script de teste no `package.json`:

```json
// package.json
{
  ...
  "scripts": {
    "test": "jest --coverage"
  },
  ...
}
```
Ao rodar `npm test` novamente, o Jest vai gerar um relatório detalhado mostrando a porcentagem do seu código que está sendo coberta pelos testes. Isso é crucial para entender quão "testado" seu código realmente está.

É isso, pessoal! Vocês deram os primeiros passos sólidos em testes automatizados com Jest. Lembrem-se: testes não são um custo, são um **investimento** na qualidade e na sustentabilidade do seu software. Continuem explorando e testando sem medo! E qualquer dúvida, podem me procurar, porque aqui no Curso.Dev, a gente não deixa ninguém para trás. Bons códigos!


Dia 16: 

🚗 Pista Rápida: Dia 16

O Dia 16 é mais um daqueles dias especiais aqui no curso.dev porque mistura parte teórica, parte prática e também como ser um profissional melhor. E eu diria que esta última parte de ser um profissional melhor é o que faz grudar na sua mente a parte teórica e prática, tanto que no final dessa Pista Rápida aqui eu adicionei uma mensagem extra que eu não coloquei em nenhuma Pista Lenta 💪

Anotações:

Este dia focou em como garantir a qualidade e a comunicação em software, culminando em uma lição fundamental sobre como abordar a tecnologia.

1. Estratégia de Testes: Garantindo a Qualidade
A qualidade do software é verificada através de diferentes tipos de testes, organizados estrategicamente.

Pirâmide de Testes: É o modelo clássico para pensar em testes.
  * Base (Testes Unitários): Testam a menor parte do código (uma função, um componente) de forma isolada. São rápidos, baratos e devem existir em grande quantidade.
  
  * Meio (Testes de Integração): Verificam se diferentes unidades do sistema funcionam corretamente juntas. Exemplo: testar se a API, ao ser chamada, consegue de fato gravar os dados no banco de dados. São mais lentos e complexos que os testes unitários.
  
  * Topo (Testes de Ponta-a-Ponta): Simulam a jornada completa do usuário na aplicação. São os mais lentos e caros.

Alternativas: Existem outros modelos além da pirâmide (como o "favo de mel"), mas o princípio de ter uma estratégia com diferentes camadas de teste permanece.

2. APIs: A Linguagem da Comunicação entre Sistemas
Os testes de integração frequentemente validam a comunicação entre diferentes partes de um sistema, que ocorre via APIs.

API (Interface de Programação de Aplicações): É um contrato que define como um software "conversa" com outro. O conceito de "interface" é universal: pode ser uma interface gráfica (para um humano) ou uma interface programática (para outro sistema).

Protocolo HTTP: É o conjunto de regras que possibilita a comunicação em APIs na web (requisições e respostas).

curl: Uma ferramenta de linha de comando essencial para interagir e testar APIs diretamente, enviando requisições HTTP sem a necessidade de um navegador.

Breaking Changes: Uma mudança em uma API que quebra a integração para quem já a utiliza (ex: renomear um campo). Exige cuidado e versionamento (v1 -> v2).

Non-Breaking Changes: Uma mudança que não quebra a integração (ex: adicionar um novo campo opcional).

3. A Mentalidade Chave: Nada é Mágico
Esta é a lição que conecta tudo e eleva um profissional:

Entender como as tecnologias funcionam "por dentro" é mais importante do que apenas usá-las.

Achar que as ferramentas e processos funcionam por "magia" limita sua capacidade de resolver problemas complexos. Um profissional de destaque se diferencia pela curiosidade de desmontar a "caixa-preta", seja para entender melhor os protocolos de comunicação (HTTP), as ferramentas de teste (curl) ou as estratégias de qualidade (Pirâmide de Testes). Essa profundidade de conhecimento é o que realmente solidifica o aprendizado e a sua eficácia.





A maior briga no universo dos Testes Automatizados
Testes Automatizados é um assunto tão importante quanto ele é passível de gerar briga na internet e nessa aula eu vou mostrar o motivo. É muito importante você estar preparado para esse tipo de discussão, principalmente sobre a diferença entre testes unitários, integração e e2e 💪

Pergunta
Qual a sua definição sobre os tipos de teste que existem?

Resposta
Acredito que o meio termo seja o caminho como foi ensinado na aula.

Anotações:

Issue: Banco de dados (homologação e produção) #11

-[] Criar endpoint `/status`
-[] Subir Banco de Dados (Local)
-[] Criar módulo `database.js`

O debate sobre qual tipo de teste é "melhor" é comum porque cada um oferece um tipo diferente de confiança e possui custos distintos. A chave, como visto na aula, é encontrar um equilíbrio que funcione para o projeto, em vez de seguir um dogma.

1. A Hierarquia dos Testes

Pirâmide de Testes (Modelo Clássico): Propõe uma base larga de testes rápidos e baratos, e um topo estreito de testes lentos e caros.
  * Unit (Unitários): Testa uma única função ou componente de forma isolada. Rápido, mas não garante que o sistema funciona como um todo.
  
  * Integration (Integração): Testa como duas ou mais partes do sistema interagem. Ex: Testa se a API consegue se comunicar com o banco de dados. Fornece mais confiança que um teste unitário.

  * E2E (Ponta-a-Ponta): Simula a jornada completa do usuário através da interface gráfica (UI). É o teste que dá mais confiança, porém é o mais lento, caro e frágil.

2. Alternativas e a Tendência "API First"

Troféu de Teste / Favo de Mel: São modelos alternativos que questionam a hegemonia dos testes unitários e dão maior ênfase aos testes de integração.

API First: Uma abordagem que prioriza o desenvolvimento e o teste da API. Nesse cenário, os testes de integração se tornam o foco principal, pois garantem que os contratos da API e suas conexões com outros serviços (como o banco de dados) são confiáveis.

3. Passos Práticos para um Teste de Integração
As anotações práticas são um exemplo de como construir um teste de integração para um endpoint que verifica a "saúde" da conexão com o banco de dados:

  * Criar o módulo database.js: Isola a lógica de conexão com o banco de dados.

  * Subir o Banco de Dados (Local): Preparar o ambiente para que o teste possa interagir com um banco de dados real.

  * Criar o endpoint /status: Um endpoint na API que, ao ser acessado, irá utilizar o módulo database.js para verificar o status da conexão, provando que a integração entre a API e o banco está funcionando.




Encostando a mão no Protocolo HTTP 🔥
Essa aula vai ser muito massa, porque eu e você vamos encostar a mão no protocolo HTTP e isso não somente vai esclarecer na sua mente muita coisa sobre tudo o que existe na internet, como também vai ser a base para conseguir entender de fato (ou revisitar) muita coisa importante como: cookies, cabeçalhos, status codes, mas tirando toda a magia de onde esses dados vem, e tudo isso vai colocar você um passo mais próximo da senioridade, ou pelo menos, vai fazer você conseguir ter conversas sérias, muito mais avançadas e certeiras quando o contexto pedir por uma pessoa que tenha maturidade e experiência nesse assunto.

Anotações:

O objetivo desta aula é remover a "magia" do funcionamento da internet, entendendo na prática como o protocolo HTTP funciona. Esse conhecimento é a base para dominar tópicos como APIs, cabeçalhos e status codes, e é um passo fundamental para a senioridade técnica.

1. O Papel das Interfaces e Abstrações
API (Interface de Programação de Aplicações): O princípio fundamental é que "tudo é uma interface". Seja uma interface de usuário (gráfica) ou uma interface programável (API), o objetivo é o mesmo.

Abstração: Interfaces são uma forma de abstração. Elas nos permitem usar uma funcionalidade sem precisar conhecer os detalhes de sua implementação interna. Elas definem um "contrato" de como interagir com um sistema.

2. Criando um Endpoint de API com Next.js
File-based Routing: No Next.js, o roteamento é baseado em arquivos. Ao criar um arquivo dentro da pasta /pages/api/, ele automaticamente se torna um endpoint de API público.

Exemplo (/pages/api/status.js): Um arquivo que exporta uma função se torna um "ponto final" onde um cliente pode fazer uma requisição.

``` JavaScript

// A função recebe dois objetos principais:
// request: Contém os dados da requisição que chega.
// response: Contém os métodos para construir e enviar a resposta.
function status(request, response) {
  // .status(200) define o HTTP Status Code para "OK".
  // .json({}) envia uma resposta no formato JSON.
  response.status(200).json({ status: "ok" });
}

export default status;

```

3. A Diferença entre .send() e .json()
O método .send() envia os dados, mas não especifica o tipo de conteúdo ou o charset.

O método .json() é mais inteligente: ele automaticamente define o cabeçalho (Header) da resposta para Content-Type: application/json; charset=utf-8, informando ao cliente como interpretar os dados recebidos.

4. "Encostando a Mão" no HTTP com curl
curl é uma ferramenta de linha de comando para fazer requisições HTTP, permitindo ver a comunicação "crua" entre cliente e servidor.

Comando: curl http://localhost:3000/api/status --verbose (ou -v).

A flag --verbose: Revela todos os detalhes da transação HTTP que normalmente ficam ocultos.

> (Request): Mostra os cabeçalhos que o seu cliente (curl) enviou para o servidor.

< (Response): Mostra os cabeçalhos que o servidor enviou de volta como resposta.

* (Curl Internals): Mostra informações sobre o processo de conexão que o curl está executando.

Usar o curl --verbose é a forma prática de ver os cabeçalhos (como o Content-Type) e os status codes (como o 200) em ação, transformando teoria em conhecimento concreto.



Não é magia! (é Protocolo)
Eu sugiro você ter um único objetivo com estas aulas mais recentes, que é acreditar que, na area de tecnologia ou na programação num geral, não existe magia... não existe mesmo! Toda informação está em algum lugar e você pode não conhecer esse lugar e naturalmente assumir que é um local mágico, mas não é.

Então nesta aula nós iremos cavucar um pouco mais o Protocolo HTTP na procura de informações que muitas pessoas encaram como "mágicas" 💪

Anotações:

O objetivo é provar que não existe "magia" em tecnologia. Toda informação está em algum lugar, e entender onde ela está é o que diferencia um profissional. Nesta aula, o foco é entender como um servidor compartilhado sabe qual site deve entregar para o cliente.

1. O Problema: Um Servidor, Vários Sites (Virtual Hosts)

Servidor Simples: No passado, um único endereço de IP geralmente correspondia a um único site. Acessar o IP abria o site.

Virtual Host (Servidor Compartilhado): Hoje, é comum que um único servidor (com um só IP) hospede centenas ou milhares de sites diferentes (como os da Vercel).

A "Magia": Se vários domínios (ex: siteA.com, siteB.com) apontam para o mesmo IP, como o servidor sabe qual deles o usuário quer ver?

2. A Solução: O Cabeçalho Host do Protocolo HTTP
A resposta não é mágica, está dentro do protocolo HTTP.

Quando você acessa um site, seu navegador primeiro usa o DNS para descobrir o IP do servidor daquele domínio.

Em seguida, ele envia uma requisição HTTP para esse IP. Dentro dessa requisição, existe um cabeçalho (header) chamado Host.

O valor do cabeçalho Host é o domínio que você digitou (ex: Host: siteA.com).

O servidor da Vercel (ou qualquer outro com Virtual Hosts) lê este cabeçalho Host para saber qual dos múltiplos sites que ele hospeda deve ser entregue ao cliente.

3. Provando o Conceito com curl

Podemos simular esse processo manualmente para provar que é o cabeçalho Host que comanda a operação.

Comando: curl ip_do_servidor --insecure --verbose --header 'Host: nome_do_site.com'

Análise do Comando:

curl ip_do_servidor: Faz a requisição diretamente ao IP do servidor, pulando a etapa do DNS.

--header 'Host: nome_do_site.com': Aqui está o segredo. Estamos manualmente adicionando o cabeçalho HTTP, dizendo ao servidor qual site queremos acessar.

--insecure: Necessário porque o certificado de segurança (HTTPS) geralmente está associado ao domínio, e não ao IP. Sem essa flag, o curl daria um erro de certificado.

--verbose: Exibe todos os detalhes da requisição e da resposta, permitindo ver o cabeçalho Host que enviamos em ação.

Este exercício prático demonstra que a seleção do site não é mágica, mas sim o resultado da leitura de uma informação clara e acessível enviada através do protocolo HTTP.







Versionamento de API e Endpoint "/status"
A aula de hoje tem muito mais conteúdos sobre API e HTTP, mas ela um objetivo muito claro e que já está super encaminhado, que é completar a tarefa Criar endpoint /status, e em cima disto iremos aprender sobre Versionamento de API e Breaking Changes 🤝

Anotações:

Evoluir uma API é um processo delicado. É preciso adicionar novas funcionalidades sem quebrar a integração para os clientes que já a utilizam. A chave para isso é uma boa estratégia de versionamento.

1. O Contrato da API: Breaking vs. Non-Breaking Changes
Non-Breaking Change: Uma mudança aditiva, como adicionar uma nova propriedade à resposta. O consumidor deve estar preparado para receber mais dados do que o esperado sem quebrar.

Breaking Change: Uma mudança que quebra a compatibilidade com versões anteriores (ex: renomear ou remover uma propriedade). O fornecedor da API precisa de uma estratégia para introduzir essas mudanças.

Estratégias de Versionamento: As mais comuns são via URL (URI Path Versioning) ou via cabeçalho HTTP (Header Versioning). A aula foca na primeira.

2. Implementando o Versionamento via URL (Path Versioning)
A abordagem é criar um "prefixo" na URL que indica a versão da API.

Estrutura de Pastas:

Dentro da pasta /pages/api, cria-se uma pasta para a versão: /v1.

A lógica do endpoint (o antigo status.js) é movida para dentro da nova estrutura, resultando em: pages/api/v1/status/index.js.

Resultado: O endpoint, que antes era /api/status, agora se torna /api/v1/status, indicando claramente que esta é a versão 1.

3. Testando o Endpoint Versionado
É crucial garantir que a nova URL funcione como esperado.

Organização dos Testes: A estrutura de pastas dos testes de integração (/integration) deve espelhar a da API: integration/api/v1/status/get.test.js.

Código do Teste (get.test.js):

``` JavaScript

// No protocolo HTTP, é preciso especificar o método (GET, POST, etc.)
test("GET to /api/v1/status should return 200", async () => {
  const response = await fetch("http://localhost:3000/api/v1/status");
  expect(response.status).toBe(200);
});

Observação sobre async/await:
A função fetch realiza uma operação de rede, que é assíncrona (não acontece instantaneamente). Ela retorna uma Promise (promessa) de uma resposta futura.
await é usado para pausar a execução da função até que a Promise do fetch seja resolvida (ou seja, até a resposta chegar).

Toda função que usa await precisa ser declarada como async.

4. Limpeza do Projeto
"Deletar a pasta de teste unitários e o arquivo da calculadora": Este passo simboliza a evolução do projeto. O foco sai de exemplos didáticos (testes unitários de uma calculadora) e passa para testes mais realistas e de maior valor para o projeto: os testes de integração da API.






🚗 Pista Rápida: Dia 17
Além de conversarmos sobre tudo que foi feito no Dia 17, como as 3 partes que eu considero na escolha de um Banco de Dados, entender o motivo do Docker ter "dominado" o mundo das virtualizações, depois como subir e se conectar a uma instância de Postgres de forma local utilizando o Docker Compose, além de tudo isso, eu passo mais um pouco da minha visão sobre o que faz um sênior ser um sênior na nossa área 💪

Qual Banco de Dados escolher?
Eu vou começar essa aula de um jeito estranho mas... e se eu te falar que talvez eu fiz a pior escolha de qual banco de dados usar para o TabNews e eu não me arrependo nenhum pouco? 🔥

Link para issue
Segue abaixo o link para a issue que eu comentei no vídeo:

https://github.com/filipedeschamps/tabnews.com.br/issues/61

Anotações: 

A aula aborda a escolha de um banco de dados não como uma busca pela ferramenta "perfeita", mas como uma decisão estratégica. A reflexão sobre a "pior escolha" para o TabNews, da qual não há arrependimento, ilustra que a melhor tecnologia é aquela que resolve o problema e que a equipe domina.

Os 3 Componentes na Escolha de um Banco de Dados
Para tomar uma decisão "Future-Proof" (à prova de futuro), é preciso pensar em três camadas:

O SGBD (Sistema de Gerenciamento de Banco de Dados): É a escolha do banco de dados em si.

Tipos: Relacional (SQL), Não Relacional (NoSQL), Série Temporal, Espacial, etc.

Dúvida Comum: A decisão mais frequente no mercado hoje é entre SQL vs. NoSQL.
Exemplo: PostgreSQL (SQL).

A Forma de Consulta (Query): Como a sua aplicação vai "conversar" com o banco de dados.

Com ORM (Object-Relational Mapper): Ferramentas que mapeiam tabelas do banco para objetos no seu código, abstraindo a escrita de SQL.
Exemplos: Sequelize, Prisma.

Sem ORM (Driver Nativo): Usar uma biblioteca para se conectar e escrever as consultas SQL diretamente.
Exemplo: node-postgres (pacote pg).

As Migrações (Migrations): É o sistema de versionamento para a estrutura (schema) do seu banco de dados.

Função: São arquivos com instruções que modificam o banco de dados de forma controlada e rastreável (ex: criar uma tabela, adicionar uma coluna).
Exemplo de Ferramenta: node-pg-migrate.

Outros Conceitos Abordados
Docker: Foi mencionado como a ferramenta que "dominou" a virtualização por facilitar a criação de ambientes de desenvolvimento consistentes, como subir uma instância local do Postgres via Docker Compose.

A Visão Sênior: A maturidade profissional não está em sempre escolher a tecnologia da moda, mas em entender as consequências de cada escolha (SGBD, Query, Migrations) e optar por um conjunto de ferramentas que a equipe consiga manter e evoluir com segurança.






Por que o Docker dominou o mundo?
Eu vim de uma época que me dói lembrar como que era configurar os serviços num ambiente de desenvolvimento local, porque toda hora algo mágico acontecia 😅 E nesta Pista Lenta vamos conferir a evolução deste assunto, até entrarmos na época dos containers.

Anotações:

Configurar ambientes de desenvolvimento era um processo frágil e inconsistente, conhecido pelo problema do "funciona na minha máquina". O Docker resolveu isso de uma forma muito mais eficiente que as soluções anteriores.

1. O Problema Original: A Inconsistência do "Host"
No passado, o mesmo tutorial ou código poderia falhar em máquinas diferentes por pequenas variações no ambiente local (o "Host"), como:

  * Hardware diferente
  * Sistema Operacional (versão, patches de segurança)
  * Softwares instalados (antivírus, outros programas)
  * Versão da linguagem de programação
  * Até mesmo o fuso horário

2. Primeira Solução: Máquinas Virtuais (VMs)
O que são: As VMs surgiram para criar um ambiente padronizado, emulando uma máquina completa (hardware e sistema operacional) dentro da sua máquina física.

Problema: Apesar de resolverem a inconsistência, as VMs eram muito pesadas, consumindo grande quantidade de disco, memória e processamento. Ferramentas como o Vagrant tentaram facilitar seu uso, mas o problema de performance continuava.

3. A Revolução: Contêineres e o Docker
O Docker introduziu uma abordagem mais inteligente e leve, baseada em dois recursos do Kernel do Linux:

Namespaces: Permitem isolar os processos. Um processo rodando em um contêiner não consegue "enxergar" ou interferir nos processos de outro contêiner ou do próprio Host.

cgroups: Permitem controlar e limitar os recursos (CPU, memória, etc.) que cada processo pode consumir.

O Docker empacotou essas tecnologias em uma ferramenta fácil de usar.

Comparação dos Modelos: VM vs. Contêiner
A grande vantagem do Docker fica clara ao comparar as "camadas" de cada modelo:

Modelo com Máquina Virtual (VM):

Máquina (Host)
SO (Host)
Virtualizador (Hypervisor)
SO Convidado (Guest OS) <- Camada pesada e redundante
Aplicação / Banco de Dados

Modelo com Contêiner (Docker):

Máquina (Host)
SO (Host)
Container Engine (Docker)
Aplicação / Banco de Dados

A Diferença Chave: Contêineres compartilham o Kernel do Sistema Operacional do Host. Eles não precisam carregar um sistema operacional inteiro como as VMs, o que os torna drasticamente mais leves, rápidos e eficientes em recursos. Essa eficiência é o principal motivo pelo qual o Docker dominou o mundo da virtualização e desenvolvimento.






Subir Banco de Dados (Local)
A aula de hoje vai ser bastante prática, pois iremos usar o docker compose para subir um Banco de Dados na sua versão Local 🎉

Links
Página oficial do Postgres no Docker Hub

Anotações:

O objetivo desta aula prática é utilizar o Docker Compose para criar e gerenciar um contêiner com um banco de dados PostgreSQL, simplificando radicalmente a configuração do ambiente de desenvolvimento local.

1. Conceitos Fundamentais
Dockerfile vs. Imagem vs. Contêiner:

Dockerfile: É o "código-fonte", um arquivo de texto com instruções para construir uma imagem.

Imagem: É o "binário" ou "pacote" gerado a partir de um Dockerfile. Fica salvo e pode ser compartilhado.

Contêiner: É uma instância em execução de uma imagem.

Docker Hub: É um repositório público de imagens Docker, similar ao GitHub para código. É de lá que vamos baixar a imagem oficial do postgres.

Docker Compose: Uma ferramenta para definir e rodar aplicações Docker com múltiplos contêineres (como um banco de dados e um Mailcatcher) a partir de um único arquivo de configuração.

2. Passos Práticos para Subir o Banco de Dados
Passo 1: Verificar as Ferramentas
Antes de começar, verifique se o Docker e o Docker Compose estão instalados com os comandos:

``` Bash

docker --version
docker-compose version

```

Passo 2: Criar o Arquivo de Configuração
Crie um arquivo na raiz do seu projeto chamado compose.yaml (ou docker-compose.yml).

Passo 3: Definir o Serviço do Banco de Dados
Dentro do arquivo compose.yaml, defina o serviço do banco de dados:

``` YAML

services:
  database:
    image: "postgres:16-alpine"
    environment:
      # Define a senha do superusuário do Postgres
      POSTGRES_PASSWORD: "local_password"

```

services: Inicia a declaração dos contêineres que o Compose vai gerenciar.

database: Um nome que você escolhe para o seu serviço de banco de dados.

image: "postgres:16-alpine": Especifica qual imagem usar do Docker Hub.

postgres: O nome da imagem oficial.

16-alpine: A "tag", indicando a versão 16 do Postgres na sua variante "alpine", que é mais leve.

environment: Permite passar variáveis de ambiente para dentro do contêiner. POSTGRES_PASSWORD é uma variável obrigatória exigida pela imagem do Postgres para a configuração inicial.

Passo 4: Iniciar o Contêiner
No terminal, na mesma pasta onde está o arquivo compose.yaml, execute o comando:

``` Bash

docker compose up

```

Este comando irá ler o arquivo, baixar a imagem do Postgres (se ainda não tiver), criar e iniciar o contêiner do banco de dados com as configurações definidas. Ao final do processo, você terá um banco de dados PostgreSQL rodando localmente, isolado e pronto para ser usado pela sua aplicação.




Se conectando no Banco de Dados (Local)
Eu deixei você na cara do gol na aula anterior, com o container rodando e servindo o Postgres, mas não sendo possível se conectar nele. E olha só que engraçado, a aula de hoje vai começar tentando se conectar nele mesmo assim para entender de verdade porque não é possível, e por fim, se conectar nele com sucesso 🎉

Link para o commit feito na aula
add compose config file with database service

Anotações:

O tema central é que "conhecer o problema é conhecimento". Entender por que algo falha é uma etapa crucial do aprendizado. Nesta aula, o objetivo é rodar um contêiner de banco de dados e conseguir se conectar a ele a partir da sua máquina (Host).

1. Rodando o Contêiner e Verificando seu Estado
Subindo o Contêiner: O comando docker compose up inicia os serviços. Por padrão, ele "prende" o seu terminal.

Para rodar em segundo plano (detached mode), use: docker compose up -d.

Verificando Contêineres:
docker ps: Lista os contêineres que estão em execução.

docker ps -a (ou --all): Lista todos os contêineres, inclusive os que pararam.

Inspecionando Logs: Se algo der errado, os logs são o primeiro lugar para procurar.

docker logs nome_do_contêiner: Exibe a saída de um contêiner específico.

Exit Codes: Assim como os Status Codes HTTP (200=sucesso, 500=erro), os contêineres usam Exit Codes para indicar seu estado final.

Exit Code 0: O processo terminou com sucesso.
Exit Code diferente de 0: O processo terminou com erro.

2. A Tentativa de Conexão (e o Erro Esperado)
O objetivo é se conectar ao Postgres dentro do contêiner usando o cliente psql.

Instalar o Cliente: Primeiro, é preciso ter o cliente do PostgreSQL na máquina Host.

``` Bash

sudo apt update
sudo apt install postgresql-client

```

Tentar a Conexão: O comando psql precisa saber o endereço, porta e credenciais.
 
``` Bash

psql --host=localhost --port=5432 --username=postgres

```

O Problema: Este comando vai falhar. O motivo é que, por padrão, o Docker não expõe as portas de um contêiner para a máquina Host. O "mundo de fora" não consegue acessar o que está rodando "lá dentro".

3. A Solução: Expor a Porta no compose.yaml
Para corrigir o problema, é preciso mapear uma porta da sua máquina (Host) para uma porta do contêiner.

Adicionar ao compose.yaml:

``` YAML

services:
  database:
    # ... outras configurações ...
    ports:
      # Mapeia a porta 5432 do Host para a 5432 do Contêiner
      - "5432:5432"

```

Recriar o Contêiner: As mudanças no compose.yaml só têm efeito se o contêiner for recriado.

``` Bash

# Opção 1: Parar tudo e subir de novo
docker compose down
docker compose up -d

# Opção 2: Forçar a recriação diretamente
docker compose up -d --force-recreate

```

Sucesso! Agora o comando psql irá funcionar. Você pode testar com uma consulta simples (SELECT 1 + 1;) e sair com \q.

4. Organização e Boas Práticas
Parar os Serviços: Para parar e remover os contêineres, use docker compose down.

Organizar Arquivos: Mova o compose.yaml para uma pasta dedicada, como infra/.

Usar Arquivo Específico: Ao rodar os comandos, especifique o caminho do arquivo.

``` Bash

docker compose -f infra/compose.yaml up

```










🚗 Pista Rápida: Dia 18
Como sempre, vamos passar rapidamente pelas principais coisas que aconteceram no Dia 18 (e aconteceram muitas coisas), mas tem algo em especial que eu não deixei claro em nenhuma Pista Lenta que eu quero falar aqui nessa Pista Rápida e que aconteceu 25 vezes 😍

Criar módulo "database.js"
Nesta aula iremos criar o módulo database.js que é uma abstração da nossa infraestrutura e que vai ser responsável por abrir conexão com o Banco de Dados e enviar queries pra ele. Para isto, vamos instalar o módulo pg na versão 8.11.3 🤝

Comentário destaque ⭐️
Depois de ver a aula, sugiro ler este comentário que fiz sobre a dúvida de outro aluno, pois pode ajudar a clarear alguns pontos importantes sobre a utilidade do database.js 💪

Anotações:

O objetivo da aula é criar uma abstração para o acesso ao banco de dados. Em vez de espalhar a lógica de conexão por todo o código, ela será centralizada em um único módulo (database.js), tornando a aplicação mais limpa, segura e fácil de manter.

1. Preparação do Ambiente
Instalar o Driver do Postgres: Para que o Node.js possa se comunicar com o PostgreSQL, é preciso instalar o driver correspondente.

``` Bash

npm install pg@8.11.3

``` 

Iniciar os Serviços:
Subir o banco de dados em segundo plano:

``` Bash

docker compose -f infra/compose.yaml up -d

``` 

Iniciar a aplicação em modo de desenvolvimento:

``` Bash

npm run dev

``` 

Ajuste nos Testes: Para garantir que todos os testes sejam re-executados ao salvar um arquivo, o script de teste no package.json é alterado para:

``` JSON

"test:watch": "jest --watchAll"

``` 

2. Construindo o Módulo database.js
Dentro da pasta infra, é criado o arquivo database.js. Ele será o único lugar do sistema que sabe como se conectar e executar comandos no banco de dados.

Código do infra/database.js:

``` JavaScript

import { Client } from 'pg';

async function query(queryObject) {
  const client = new Client({
    host: 'localhost',
    port: 5432,
    user: 'postgres',
    database: "postgres",
    password: "local_password"
  });
  
  await client.connect();
  const result = await client.query(queryObject);
  await client.end();
  
  return result;
}

export default {
  query: query,
};

``` 

new Client({...}): Cria uma nova instância do cliente Postgres com as credenciais de acesso, que devem ser as mesmas definidas no compose.yaml.

client.connect(): Abre a conexão com o banco de dados.

client.query(): Executa a consulta SQL recebida.

client.end(): Fecha a conexão. Este é um passo crucial para não deixar conexões abertas desnecessariamente.

3. Utilizando o Módulo
O endpoint /api/v1/status/index.js é modificado para usar a nova abstração.

``` JavaScript

import database from "../../../../infra/database.js";

// ... dentro da função do endpoint ...
const result = await database.query('SELECT 1 + 1 as sum;');
console.log(result.rows); // Exibe o resultado da consulta

``` 

O Grande Benefício: Agora, o endpoint de status (e qualquer outro) não precisa saber nada sobre o pacote pg ou sobre como abrir e fechar conexões. 

Ele apenas chama database.query(). Se no futuro for preciso trocar o driver, as credenciais ou a lógica de conexão, a mudança será feita em um único lugar: o arquivo database.js.



A importância das Variáveis de Ambiente
Quando eu realmente entendi o poder e a simplicidade das Variáveis de Ambiente, o meu cérebro deu alguns cliques muito importantes. Um deles foi de sempre que possível deixar a camada da aplicação stateless (sem estado) e isso se conecta perfeitamente com várias formas profissionais de se construir e escalar uma aplicação 💪

Anotações:

O entendimento de Variáveis de Ambiente é um "clique" fundamental para o desenvolvimento de software profissional, pois elas são a chave para criar aplicações flexíveis, escaláveis e, principalmente, Stateless (sem estado).

1. O Que São Variáveis de Ambiente?
São configurações externas que são "injetadas" em uma aplicação dependendo do ambiente onde ela está rodando. Em vez de ter dados fixos no código (hardcoded), a aplicação os lê do seu ambiente.

O Problema que Resolvem: Uma configuração que funciona em localhost (ex: senha do banco de dados local) não vai funcionar em homologação ou produção. Variáveis de Ambiente permitem que o mesmo código funcione em todos os lugares, apenas mudando os valores de acordo com o contexto.

2. O Objetivo: Uma Aplicação "Stateless"
Stateless (Sem Estado): É um princípio de arquitetura onde a camada da aplicação não armazena nenhuma informação persistente. Ela processa uma requisição e esquece. Todo o "estado" (dados de usuários, sessões, etc.) é delegado a serviços externos.

Separação de Camadas: A ideia é separar a camada de Persistência (o banco de dados) da camada de Aplicação.

Antes: Backend monolítico (Interface + Aplicação + Persistência)

Depois: Aplicação (Interface + Lógica) <-> Serviço de Banco de Dados

3. Como as Variáveis de Ambiente Conectam Tudo
As Variáveis de Ambiente são a "cola" que permite à aplicação stateless encontrar e se comunicar com os serviços de persistência.

A aplicação é programada para buscar as credenciais e endereços (como host, usuário e senha do banco) a partir das variáveis de ambiente.

Em localhost: O ambiente fornece as variáveis com os dados do seu banco de dados local.

Em Produção: O ambiente de produção (servidor, contêiner) fornece as variáveis com os dados do banco de dados de produção.

Essa abordagem torna a aplicação extremamente portátil e escalável. É possível subir múltiplas instâncias idênticas da mesma aplicação, e cada uma se conectará ao banco de dados correto simplesmente lendo as variáveis do ambiente em que foi iniciada.



Variáveis de Ambiente no Código
Qual a relação entre Variáveis de Ambiente, process, env e o que entra dentro do seu código? Vamos ver tudo isso dentro dessa aula e deixar sua aplicação 100% stateless 💪

Anotações:

O objetivo é externalizar todas as configurações sensíveis ou que mudam entre ambientes (desenvolvimento, produção), tornando a aplicação 100% stateless e segura.

1. Acessando Variáveis de Ambiente no Node.js
process.env: Em Node.js, todas as variáveis de ambiente do processo atual ficam disponíveis em um objeto global chamado process.env.

Aplicação no Código: Para usar uma variável, basta acessá-la como uma propriedade deste objeto. No arquivo database.js, os valores fixos ("hardcoded") são substituídos:

Antes:

``` JavaScript

password: "local_password",

```

Depois:

``` JavaScript

password: process.env.POSTGRES_PASSWORD,

``` 

2. Como Fornecer as Variáveis para a Aplicação
Existem duas formas principais para o ambiente de desenvolvimento local:

Método 1: Pela Linha de Comando
Você pode definir a variável diretamente no comando que inicia a aplicação. Ela será válida apenas para a execução daquele comando.

``` Bash

POSTGRES_PASSWORD=local_password npm run dev

``` 

Dica de Histórico: Iniciar um comando com um espaço em branco ("comando") geralmente evita que ele seja salvo no histórico do terminal (history), útil para não expor senhas.

Método 2: Usando um Arquivo .env (com dotenv)
Esta é a abordagem mais comum e organizada para desenvolvimento local.

Criar o arquivo .env: Na raiz do projeto, crie um arquivo chamado .env. Este arquivo nunca deve ser enviado para o repositório (Git), pois contém informações sensíveis.

Adicionar as Variáveis: Dentro do .env, liste todas as variáveis de configuração no formato CHAVE=VALOR.

``` Code snippet

POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_DB=postgres
POSTGRES_PASSWORD=local_password

``` 

(Nota: Para que isso funcione, a biblioteca dotenv precisa ser instalada e configurada no ponto de entrada da sua aplicação).

3. Resultado Final no database.js
Com as variáveis definidas no ambiente (seja via comando ou .env), o código do database.js fica limpo, seguro e flexível, lendo todas as suas configurações do process.env:

``` JavaScript

new Client({
  host: process.env.POSTGRES_HOST,
  port: process.env.POSTGRES_PORT,
  user: process.env.POSTGRES_USER,
  database: process.env.POSTGRES_DB,
  password: process.env.POSTGRES_PASSWORD,
});

``` 





Variáveis de Ambiente no Docker Compose
Como fazer para evitar de ter Variáveis de Ambiente duplicadas no arquivo compose.yaml, no arquivo databse.js e fazer tudo puxar do .env? É isso o que iremos ver nesta aula, fora se deparar com um mistério... vamos ver se você sabe a resposta 🤝

Mistério: Por quê o Banco de Dados rodou?
Por que no minuto 02:22 da aula, ao trocar a Variável de Ambiente de POSTGRES_DATABASE para POSTGRES_DB no arquivo .env, a conexão com o Banco de Dados continuou funcionando, se o database.js estava pedindo ainda pela antiga Variável de Ambiente POSTGRES_DATABASE? Você vai precisar investigar para encontrar esta resposta 💪

Comentário em destaque
Caso queira saber qual a resposta, sugiro ler esse comentário que está bastante completo 🤝

Link para o commit feito na aula
add database.js and .env files

Anotações: 
O objetivo desta aula é eliminar a duplicação de configurações, fazendo com que tanto a aplicação (database.js) quanto a infraestrutura (compose.yaml) utilizem o arquivo .env como a única fonte da verdade para as variáveis de ambiente.

1. O Problema: Configurações Duplicadas
Manter as mesmas variáveis de ambiente (como a senha do banco) hardcoded no compose.yaml e também sendo lidas via process.env na aplicação é ineficiente e propenso a erros.

2. A Solução: Usar env_file no Docker Compose
O Docker Compose possui uma diretiva chamada env_file que o instrui a carregar as variáveis de ambiente para um serviço diretamente de um arquivo, como o .env.

Passos da Implementação:

Padronizar Variáveis: Garanta que os nomes das variáveis estejam consistentes. Por exemplo, altere POSTGRES_DATABASE para POSTGRES_DB tanto no .env quanto no database.js.

Atualizar o compose.yaml: Remova as variáveis de ambiente definidas manualmente e aponte para o arquivo .env.

``` YAML

services:
  database:
    image: "postgres:16-alpine"
    # A seção "environment" foi removida
    env_file:
      - "../.env" # Carrega as variáveis do .env na raiz do projeto
    ports:
      - "5432:5432"

``` 

Aplicar as Mudanças: Reinicie os contêineres para que a nova configuração seja aplicada.

``` Bash

docker compose -f infra/compose.yaml up -d --force-recreate

``` 

3. O Mistério: Por que a Conexão Continuou Funcionando?
O Cenário: Em um momento da aula, o .env foi alterado para POSTGRES_DB, mas o database.js ainda esperava a variável antiga (POSTGRES_DATABASE), que agora estava indefinida. Mesmo assim, a conexão com o banco de dados funcionou.

A Explicação (Não é magia!):
Persistência de Dados do Docker: Por padrão, o Docker cria um "volume" para o contêiner do Postgres, onde os dados do banco são salvos. Isso evita que você perca tudo ao reiniciar o contêiner.

Inicialização do Postgres: As variáveis POSTGRES_DB e POSTGRES_USER são usadas pela imagem do Postgres apenas na primeira vez que o contêiner é criado, para inicializar o banco de dados.

Reutilização do Volume: Ao reiniciar o contêiner (docker compose up), ele detectou o volume com os dados já existentes e simplesmente ignorou as variáveis de inicialização (POSTGRES_DB), pois o banco já estava criado. A conexão continuou funcionando com a base de dados que já existia.

4. Próximos Passos no Fluxo de Trabalho
Commit: Após as alterações, salvar o progresso no Git (git commit).

Atualizar a Issue: Criar uma nova tarefa no quadro de trabalho, como - [ ] Finalizar retorno do endpoint /status, para dar continuidade ao desenvolvimento.